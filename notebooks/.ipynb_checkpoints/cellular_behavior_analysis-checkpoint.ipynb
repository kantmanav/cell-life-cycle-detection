{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "signed-thinking",
   "metadata": {},
   "source": [
    "# Cellular Behavior Analysis\n",
    "This notebook uses autoencoders to explore dynamic cellular behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fluid-berkeley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import deepcell\n",
    "from deepcell_tracking.utils import Track, load_trks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepcell.utils.data_utils import reshape_movie\n",
    "from deepcell.utils.transform_utils import erode_edges\n",
    "from deepcell.data import split_dataset\n",
    "from deepcell_toolbox.processing import normalize, histogram_normalization\n",
    "\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-grounds",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_temp_trks = '/data/3T3_nuc_s0-s2.trks'\n",
    "all_data = [load_trks(path_temp_trks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_tracks = [Track(tracked_data=d) for d in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from deepcell_tracking.utils import concat_tracks\n",
    "track_info = concat_tracks(all_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in track_info.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-bacteria",
   "metadata": {},
   "source": [
    "## Convert data to tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def norm(X):\n",
    "    X_norm = histogram_normalization(X, kernel_size=32)\n",
    "    \n",
    "    X_rescaled = np.zeros(X_norm.shape)\n",
    "    for batch in range(X.shape[0]):\n",
    "        for frame in range(X.shape[1]):\n",
    "            x = X[batch, frame]\n",
    "            x = rescale_intensity(x, out_range=(0,1))\n",
    "            X_rescaled[batch, frame] = x\n",
    "            \n",
    "    return X_rescaled\n",
    "\n",
    "def prepare_dataset(track_info, batch_size=1, buffer_size=256,\n",
    "                    seed=None, val_split=0.2):\n",
    "    \n",
    "    # Merge tracks along the batch axis\n",
    "    appearances = track_info['appearances']\n",
    "    \n",
    "    app_shape = np.shape(appearances)\n",
    "    appearances = np.swapaxes(appearances, 1,2) #(0,2,1,3,4,5))\n",
    "    appearances = np.reshape(appearances, [-1, app_shape[1], app_shape[3], app_shape[4], app_shape[5]])\n",
    "    \n",
    "    # Normalize appearances\n",
    "    appearances = norm(appearances)\n",
    "    \n",
    "    input_dict = {'appearances': appearances}\n",
    "    \n",
    "    output_dict = {'appearances_decoder': appearances}\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_dict, output_dict))\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size, seed=seed)\n",
    "    \n",
    "    # split into train/val\n",
    "    train_data, val_data = split_dataset(dataset, val_split)\n",
    "    \n",
    "    # batch the data\n",
    "    train_data = train_data.repeat().batch(batch_size)\n",
    "    val_data = val_data.repeat().batch(batch_size)\n",
    "    \n",
    "    # prefetch the data\n",
    "    train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "    val_data = val_data.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480e960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, val_data = prepare_dataset(track_info,\n",
    "                                       batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,20,figsize=(20,10))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "nxt = it.next()\n",
    "for j in range(71):\n",
    "    axes.flatten()[j].imshow(nxt[0]['appearances'][0,j])\n",
    "    axes.flatten()[j].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-french",
   "metadata": {},
   "source": [
    "## Construct autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv3D, LSTM, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Concatenate, InputLayer\n",
    "from tensorflow.keras.layers import Add, Subtract, Dense, Reshape\n",
    "from tensorflow.keras.layers import MaxPool3D, UpSampling3D\n",
    "from tensorflow.keras.layers import Activation, Softmax\n",
    "from tensorflow.keras.layers import BatchNormalization, Lambda\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "\n",
    "in_kwargs = {'axis':-1,\n",
    "             'center': True,\n",
    "             'scale': True,\n",
    "             'beta_initializer': 'random_uniform',\n",
    "             'gamma_initializer': 'random_uniform'}\n",
    "\n",
    "def residual_block(input_tensor, n_filters=64):\n",
    "    y = Conv3D(n_filters,\n",
    "           (1, 3, 3),\n",
    "           strides=1,\n",
    "           padding='same')(input_tensor)\n",
    "    y = InstanceNormalization(**in_kwargs)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    y = Conv3D(n_filters,\n",
    "           (1, 3, 3),\n",
    "           strides=1,\n",
    "           padding='same')(y)\n",
    "    y = InstanceNormalization(**in_kwargs)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    y = Add()([input_tensor, y])\n",
    "    \n",
    "    return y\n",
    "    \n",
    "class TrajectoryEncoder():   \n",
    "    def __init__(self,\n",
    "                 n_filters = 64,\n",
    "                 encoder_dim=256,\n",
    "                 embedding_dim=1024,\n",
    "                 appearance_shape=(71,32,32,1)):\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.appearance_shape = appearance_shape\n",
    "        \n",
    "        # create encoder\n",
    "        self.app_encoder = self.get_appearance_encoder()\n",
    "        \n",
    "        # create embedding\n",
    "        self.embedding_model = self.get_embedding_model()\n",
    "        \n",
    "        # create decoder\n",
    "        self.app_decoder = self.get_appearance_decoder()\n",
    "        \n",
    "        # create autoencoder\n",
    "        self.autoencoder = self.get_autoencoder()\n",
    "    \n",
    "    def get_appearance_encoder(self):\n",
    "        app_shape = tuple([None] + list(self.appearance_shape)[1:])\n",
    "        inputs = Input(shape=app_shape, name='encoder_app_input')\n",
    "\n",
    "        x = inputs\n",
    "        x = Conv3D(self.n_filters,\n",
    "                       (1,3,3),\n",
    "                       strides=1,\n",
    "                       padding='same')(x)\n",
    "        \n",
    "        for i in range(5): \n",
    "            x = residual_block(x, self.n_filters)\n",
    "            x = residual_block(x, self.n_filters)\n",
    "\n",
    "            x = Conv3D(self.n_filters, \n",
    "                       (1,2,2), \n",
    "                       strides=(1,2,2), \n",
    "                       padding='valid',\n",
    "                       use_bias=False)(x)\n",
    "            \n",
    "        x = Lambda(lambda t: tf.squeeze(t, axis=(2, 3)))(x)\n",
    "        x = Dense(self.encoder_dim, name='dense_aeout')(x)\n",
    "        x = Activation('relu', name='appearance_embedding')(x)\n",
    "        return Model(inputs=inputs, outputs=x, name='appearance_encoder')\n",
    "        \n",
    "    def get_embedding_model(self):\n",
    "        encoded_shape = tuple([None, self.encoder_dim])\n",
    "        inputs = Input(shape=encoded_shape, name='embedding_model_input')\n",
    "        \n",
    "        x = inputs\n",
    "        x = Dense(self.embedding_dim)(x)\n",
    "        x = Activation('relu', name='embedding')(x)\n",
    "        return Model(inputs=inputs, outputs=x, name='embedding')\n",
    "    \n",
    "    def get_appearance_decoder(self):\n",
    "        embedding_shape = tuple([None, self.embedding_dim])\n",
    "        inputs = Input(shape=embedding_shape, name='encoder_app_input')\n",
    "\n",
    "        x = inputs\n",
    "        x = Lambda(lambda t: tf.expand_dims(t, axis=-2))(x)\n",
    "        x = Lambda(lambda t: tf.expand_dims(t, axis=-2))(x)\n",
    "        \n",
    "        x = Conv3D(self.n_filters,\n",
    "               (1,3,3),\n",
    "               strides=1,\n",
    "               padding='same')(x)\n",
    "        \n",
    "        for i in range(5): \n",
    "            x = residual_block(x, self.n_filters)\n",
    "            x = residual_block(x, self.n_filters)\n",
    "            x = UpSampling3D(size=(1, 2, 2))(x)\n",
    "            \n",
    "        x = Dense(1, name='appearance_reconstruction')(x)\n",
    "        return Model(inputs=inputs, outputs=x, name='appearances_decoder')\n",
    "        \n",
    "    def get_autoencoder(self):\n",
    "        app_input = Input(shape=self.appearance_shape, name='appearances')\n",
    "\n",
    "        x = self.app_encoder(app_input)\n",
    "        embedding = self.embedding_model(x)\n",
    "        reconstruction = self.app_decoder(embedding)\n",
    "\n",
    "        inputs = app_input\n",
    "        outputs = [embedding, reconstruction]\n",
    "        return Model(inputs=inputs, outputs=[embedding, reconstruction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "TE = TrajectoryEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(TE.autoencoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_supervised_loss(y_true, y_pred):\n",
    "    \n",
    "    # Pair the current embedding with the future ground truth\n",
    "    # so that we predict the future from the past\n",
    "    \n",
    "    current_reconstruction = y_pred\n",
    "    gt = y_true\n",
    "    \n",
    "    loss = tf.keras.metrics.mean_squared_error(gt, current_reconstruction)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-marathon",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "from deepcell import train_utils\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(lr=1e-3, clipnorm=0.001)\n",
    "losses = {'appearances_decoder': self_supervised_loss}\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))\n",
    "\n",
    "# Compile model\n",
    "TE.autoencoder.compile(optimizer=optimizer, loss=losses)\n",
    "\n",
    "# Train the model\n",
    "model_path = '/data/models/ssl'\n",
    "steps_per_epoch = 2048\n",
    "validation_steps = 100\n",
    "n_epochs = 1\n",
    "\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "loss_history = TE.autoencoder.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170243f",
   "metadata": {},
   "source": [
    "## Visualize reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85056a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a135639",
   "metadata": {},
   "outputs": [],
   "source": [
    "nxt = it.next()\n",
    "inputs = nxt[0]['appearances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,20,figsize=(20,10))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "for j in range(71):\n",
    "    axes.flatten()[j].imshow(inputs[0,j])\n",
    "    axes.flatten()[j].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31125bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_encoded = TE.app_encoder(inputs)\n",
    "embedding = TE.embedding_model(app_encoded)\n",
    "reconstruction = TE.app_decoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,20,figsize=(20,10))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "for j in range(71):\n",
    "    axes.flatten()[j].imshow(reconstruction[0,j])\n",
    "    axes.flatten()[j].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-flush",
   "metadata": {},
   "source": [
    "## Visualize embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "surprised-peace",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-29d945e74abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "outputs = TE.autoencoder.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applicable-diamond",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3f30b2e4aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TE' is not defined"
     ]
    }
   ],
   "source": [
    "TE.autoencoder.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-matrix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
